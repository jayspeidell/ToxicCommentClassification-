{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kaYNMVqj5yxo"
   },
   "source": [
    "## Preface\n",
    "\n",
    "This is an analysis of Wikipedia comments to create models that identify various types of toxic comments. There is a lot of racist content and swear words in the dataset and some of it will pop up in the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 920,
     "output_extras": [
      {
       "item_id": 23
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32492,
     "status": "ok",
     "timestamp": 1522886463906,
     "user": {
      "displayName": "Jay Speidell",
      "photoUrl": "//lh6.googleusercontent.com/-PA283dAAXaQ/AAAAAAAAAAI/AAAAAAAAGng/nfaP1-4Q3qM/s50-c-k-no/photo.jpg",
      "userId": "112011094715710279880"
     },
     "user_tz": 420
    },
    "id": "It7csY1f5yxs",
    "outputId": "a415ec57-e23e-4d58-b3bf-b46882562dbc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import keras\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from collections import namedtuple\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "from sklearn.pipeline import make_union\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global random state and k-fold strategy \n",
    "seed = 42\n",
    "k = 5\n",
    "cv = StratifiedKFold(n_splits=k, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_f1_score(y_hat, data):\n",
    "    # https://stackoverflow.com/questions/49774825/python-lightgbm-cross-validation-how-to-use-lightgbm-cv-for-regression\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) \n",
    "    return 'f1', f1_score(y_true, y_hat), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "keYiW4Ws5yx2"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "def print_time(start):\n",
    "    time_now = time.time() - start \n",
    "    minutes = int(time_now / 60)\n",
    "    seconds = int(time_now % 60)\n",
    "    if seconds < 10:\n",
    "        print('Elapsed time was %d:0%d.' % (minutes, seconds))\n",
    "    else:\n",
    "        print('Elapsed time was %d:%d.' % (minutes, seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, sparse=0): \n",
    "    \n",
    "    # Comment length\n",
    "    df['length'] = df.comment_text.apply(lambda x: len(x))\n",
    "    \n",
    "\n",
    "    # Capitalization percentage\n",
    "    def pct_caps(s):\n",
    "        return sum([1 for c in s if c.isupper()]) / (sum(([1 for c in s if c.isalpha()])) + 1)\n",
    "    df['caps'] = df.comment_text.apply(lambda x: pct_caps(x))\n",
    "\n",
    "    # Mean Word length \n",
    "    def word_length(s):\n",
    "        s = s.split(' ')\n",
    "        return np.mean([len(w) for w in s if w.isalpha()])\n",
    "    df['word_length'] = df.comment_text.apply(lambda x: word_length(x))\n",
    "\n",
    "    # Average number of exclamation points \n",
    "    df['exclamation'] = df.comment_text.apply(lambda s: len([c for c in s if c == '!']))\n",
    "\n",
    "    # Average number of question marks \n",
    "    df['question'] = df.comment_text.apply(lambda s: len([c for c in s if c == '?']))\n",
    "    \n",
    "    # Normalize\n",
    "    for label in ['length', 'caps', 'word_length', 'question', 'exclamation']:\n",
    "        minimum = df[label].min()\n",
    "        diff = df[label].max() - minimum\n",
    "        df[label] = df[label].apply(lambda x: (x-minimum) / (diff))\n",
    "\n",
    "    # Strip IP Addresses\n",
    "    ip = re.compile('(([2][5][0-5]\\.)|([2][0-4][0-9]\\.)|([0-1]?[0-9]?[0-9]\\.)){3}'\n",
    "                    +'(([2][5][0-5])|([2][0-4][0-9])|([0-1]?[0-9]?[0-9]))')\n",
    "    def strip_ip(s, ip):\n",
    "        try:\n",
    "            found = ip.search(s)\n",
    "            return s.replace(found.group(), ' ')\n",
    "        except:\n",
    "            return s\n",
    "\n",
    "    df.comment_text = df.comment_text.apply(lambda x: strip_ip(x, ip))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def merge_features(comment_text, data, engineered_features):\n",
    "    new_features = sparse.csr_matrix(df[engineered_features].values)\n",
    "    if np.isnan(new_features.data).any():\n",
    "        new_features.data = np.nan_to_num(new_features.data)\n",
    "    return sparse.hstack([comment_text, new_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels:\n",
      "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "(159571, 6)\n",
      "\n",
      "Training data\n",
      "['comment_text', 'length', 'caps', 'word_length', 'exclamation', 'question']\n",
      "(159571, 6)\n",
      "\n",
      "Submission data\n",
      "['comment_text', 'length', 'caps', 'word_length', 'exclamation', 'question']\n",
      "(153164, 6)\n",
      "['length', 'caps', 'word_length', 'exclamation', 'question']\n"
     ]
    }
   ],
   "source": [
    "# Reset data and create holdout set. \n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "targets = list(df.columns[2:])\n",
    "df_targets = df[targets].copy()\n",
    "\n",
    "df_sub = pd.read_csv('test.csv', dtype={'id': object}, na_filter=False)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = df_sub.id.copy()\n",
    "\n",
    "# Feature Engineering\n",
    "df = feature_engineering(df)\n",
    "df_sub = feature_engineering(df_sub)\n",
    "\n",
    "print('Training labels:')\n",
    "print(list(df_targets.columns))\n",
    "print(df_targets.shape)\n",
    "\n",
    "print('\\nTraining data')\n",
    "df.drop(list(df_targets.columns), inplace=True, axis=1)\n",
    "df.drop('id', inplace=True, axis=1)\n",
    "print(list(df.columns))\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "print('\\nSubmission data')\n",
    "df_sub.drop('id', inplace=True, axis=1)\n",
    "print(list(df_sub.columns))\n",
    "print(df_sub.shape)\n",
    "\n",
    "toxic_rows = df_targets.sum(axis=1)\n",
    "toxic_rows = (toxic_rows > 0)\n",
    "targets.append('any_label')\n",
    "df_targets['any_label'] = toxic_rows.astype(int)\n",
    "\n",
    "new_features = list(df.columns[1:])\n",
    "print(new_features)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df, holdout, df_targets, holdout_targets = train_test_split(df, df_targets, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['length', 'caps', 'word_length', 'exclamation', 'question']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilabel Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "#todo \n",
    "# Weights for \n",
    "def multi_cv(model, data, labels, k=5, nb_features=False):\n",
    "    cv = StratifiedKFold(n_splits=k, random_state=seed)\n",
    "    # Creating NB features just once from any_label has about the same \n",
    "    # performance as individual labels with faster speed. \n",
    "    def log_count_ratio(x, y):\n",
    "        x = sparse.csr_matrix(x)\n",
    "        # WARNING: Some scipy modules use indexes that start at 1! \n",
    "        # You need to add 1 to an index when performing operations on a csr_matrix \n",
    "\n",
    "        p = abs(x[np.where(y==1)].sum(axis=0))\n",
    "        p = p + 1\n",
    "        p = p / np.sum(p)\n",
    "\n",
    "        q = abs(x[np.where(y==0)].sum(axis=0))\n",
    "        q = q + 1\n",
    "        q = q / np.sum(q)\n",
    "\n",
    "        return np.log(p/q)\n",
    "    \n",
    "    # Labels must be in a dataframe\n",
    "    scores = []\n",
    "    r_values = []\n",
    "    for label in labels.columns:\n",
    "        if nb_features:\n",
    "            r = log_count_ratio(data, labels[label])\n",
    "            r_values.append(r)\n",
    "            data = data.multiply(r)\n",
    "            if np.isnan(data.data).any():\n",
    "                data.data = np.nan_to_num(data.data)\n",
    "        score = np.mean(cross_val_score(clone(model), data, labels[label], scoring='f1', cv=cv))\n",
    "        print(label + ' f1 score: %.4f' % score)\n",
    "        scores.append(score)\n",
    "    print('Average (excluding any) f1 score: %.4f' % np.mean(scores[:-1]))\n",
    "    if nb_features:\n",
    "        return scores, r_values\n",
    "    else:\n",
    "        return scores\n",
    "\n",
    "#training_comments.data = np.nan_to_num(training_comments.data)\n",
    "\n",
    "#model = LinearSVC()\n",
    "#_ = multi_cv(model, training_comments, df_targets, nb_features=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB Feature Transformer \n",
    "\n",
    "This is the primary method that I will use for the NB-SVM models, but I've left other code in to use as a reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBFeatures:\n",
    "    def __init__(self, epsilon=1, sparse=True):\n",
    "        # How much influence NB features have \n",
    "        if not epsilon > 0 and epsilon <= 1:\n",
    "            raise Exception(\"Invalid Epsilon value. Must be greater than zero and less than or equal to one.\")\n",
    "        self.epsilon = epsilon\n",
    "        self.r = None\n",
    "    \n",
    "    def log_count_ratio(self, x, y):\n",
    "        x = sparse.csr_matrix(x)\n",
    "        # WARNING: Some scipy authors fall in the \"index starts at 1\" camp\n",
    "        # You need to add 1 to an index when performing operations on a csr_matrix \n",
    "        p = abs(x[np.where(y==1)].sum(axis=0))\n",
    "        p = p + 1\n",
    "        p = p / np.sum(p)\n",
    "        q = abs(x[np.where(y==0)].sum(axis=0))\n",
    "        q = q + 1\n",
    "        q = q / np.sum(q)\n",
    "        return np.log(p/q)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.r = self.log_count_ratio(x, y)\n",
    "    \n",
    "    def transform(self, x):\n",
    "        if self.r == None: \n",
    "            raise Exception(\"Model not fit, can't transform.\")\n",
    "        transformed = x.multiply(self.r)\n",
    "        return x.multiply(1-self.epsilon) + transformed.multiply(self.epsilon)\n",
    "        #return np.multiply(x, self.r)\n",
    "    \n",
    "    def fit_transform(self, x, y):\n",
    "        self.r = self.log_count_ratio(x, y)\n",
    "        return self.transform(x, y)\n",
    "\n",
    "\n",
    "#nb_trans = NBFeatures(0.5)\n",
    "#new = nb_trans.fit_transform(training_comments, np.array(df_targets.iloc[:,-1]))\n",
    "#nb_trans.r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A separate helper function to calculate the log count ratio that can be used for experimentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_count_ratio(x, y):\n",
    "    x = sparse.csr_matrix(x)\n",
    "    # WARNING: Some scipy authors fall in the \"index starts at 1\" camp\n",
    "    # You need to add 1 to an index when performing operations on a csr_matrix \n",
    "\n",
    "    p = abs(x[np.where(y==1)].sum(axis=0))\n",
    "    p = p + 1\n",
    "    p = p / np.sum(p)\n",
    "\n",
    "    q = abs(x[np.where(y==0)].sum(axis=0))\n",
    "    q = q + 1\n",
    "    q = q / np.sum(q)\n",
    "\n",
    "    return np.log(p/q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's necessary to vectorize text before inputting into machine learning models. This is a process of translating string data into numerical data that the computer can better understand. Vectorized data is usually sparse, with an array where the features contain either word counts or another way of representing the occurance of characters or words in a string. This is done with a vectorizer object, which stores a dictionary of characters or words and their associated integer representation, along with relevant statistics if applicable. \n",
    "\n",
    "The strategy I'm going to use here is term frequency - inverse document frequency. This is a statistic that describes the usefulness of a string of characters by looking at the frequency that it occurs in an individual document (here, a single comment) and the inverse of its frequency in all of the documents in the dataset. \n",
    "\n",
    "That means that a word that is used frequently in a comment in this dataset, but that few comments in the dataset feature, is probably useful to the model. But a string that occurs in nearly every document is almost useless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TGoGjh6b5y0i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time was 0:19.\n",
      "(127656, 10000)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "comment_vector = TfidfVectorizer(max_features=10000, analyzer='word', #ngram_range=(2, 6), \n",
    "                                 stop_words='english')\n",
    "training_comments = comment_vector.fit_transform(df.comment_text)\n",
    "holdout_comments = comment_vector.transform(holdout.comment_text)\n",
    "submission_comments = comment_vector.transform(df_sub.comment_text)\n",
    "print_time(start)\n",
    "\n",
    "print(training_comments.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important parameters to tune in this problem is the number of features and the n_gram range in the TF-IDF vectorizer, as well as choosing whether to analyze the sequences by characters or words. Analyzing by single words initially gives very poor performance, possibly because slang words and misspellings reduce the frequency of individual bad words.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple function to play with reducing class imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just experimental to learn about the behavior of models with imbalanced classes. \n",
    "\n",
    "from numpy.random import sample\n",
    "\n",
    "def imbalance_reduction(p, y):\n",
    "    \"\"\"\n",
    "    For multilabel problems, keeps all rows with a \n",
    "    positive label and returns p% of data where label is zero. \n",
    "    \"\"\"\n",
    "    #reduce y\n",
    "    y = np.sum(y, axis=1)\n",
    "    p = 1-p\n",
    "    keep_index = sample(len(y))\n",
    "    keep_index = keep_index + y\n",
    "    keep_index[keep_index>=p] = 1\n",
    "    keep_index[keep_index<p] = 0\n",
    "\n",
    "    return np.where(keep_index==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 10,000 vectorized features, but without engineered features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic score: 0.7192\n",
      "severe_toxic score: 0.3224\n",
      "obscene score: 0.7452\n",
      "threat score: 0.2069\n",
      "insult score: 0.6277\n",
      "identity_hate score: 0.2772\n",
      "any_label score: 0.7299\n",
      "Elapsed time was 0:43.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "for target in targets: \n",
    "    lr = LogisticRegression(random_state=seed)\n",
    "    print(target + ' score: %.4f' % np.mean(cross_val_score(lr, training_comments, df_targets[target], scoring='f1', cv=cv)))\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With engineered features added in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic score: 0.7235\n",
      "severe_toxic score: 0.3475\n",
      "obscene score: 0.7440\n",
      "threat score: 0.2029\n",
      "insult score: 0.6274\n",
      "identity_hate score: 0.2768\n",
      "any_label score: 0.7328\n",
      "Elapsed time was 0:54.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for target in targets: \n",
    "    lr = LogisticRegression(random_state=seed)\n",
    "    print(target + ' score: %.4f' % np.mean(cross_val_score(lr, merge_features(training_comments, df, new_features), df_targets[target], scoring='f1', cv=cv)))\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic f1 score: 0.6581\n",
      "severe_toxic f1 score: 0.1032\n",
      "obscene f1 score: 0.6672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threat f1 score: 0.0000\n",
      "insult f1 score: 0.5603\n",
      "identity_hate f1 score: 0.0418\n",
      "any_label f1 score: 0.6670\n",
      "Average (excluding any) f1 score: 0.3384\n",
      "Elapsed time was 0:02.\n"
     ]
    }
   ],
   "source": [
    "start = time.time() \n",
    "\n",
    "model = MultinomialNB(alpha=1.0)\n",
    "_ = multi_cv(model, training_comments, df_targets)\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With engineered features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic f1 score: 0.6662\n",
      "severe_toxic f1 score: 0.0992\n",
      "obscene f1 score: 0.6742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threat f1 score: 0.0000\n",
      "insult f1 score: 0.5708\n",
      "identity_hate f1 score: 0.0369\n",
      "any_label f1 score: 0.6734\n",
      "Average (excluding any) f1 score: 0.3412\n",
      "Elapsed time was 0:03.\n"
     ]
    }
   ],
   "source": [
    "start = time.time() \n",
    "\n",
    "model = MultinomialNB(alpha=1.0)\n",
    "_ = multi_cv(model, merge_features(training_comments, df, new_features), df_targets)\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic f1 score: 0.7551\n",
      "severe_toxic f1 score: 0.3382\n",
      "obscene f1 score: 0.7806\n",
      "threat f1 score: 0.3658\n",
      "insult f1 score: 0.6643\n",
      "identity_hate f1 score: 0.3562\n",
      "any_label f1 score: 0.7703\n",
      "Average (excluding any) f1 score: 0.5434\n",
      "Elapsed time was 0:25.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model = LinearSVC(random_state=seed)\n",
    "_ = multi_cv(model, training_comments, df_targets)\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic f1 score: 0.7594\n",
      "severe_toxic f1 score: 0.3492\n",
      "obscene f1 score: 0.7797\n",
      "threat f1 score: 0.3624\n",
      "insult f1 score: 0.6643\n",
      "identity_hate f1 score: 0.3548\n",
      "any_label f1 score: 0.7739\n",
      "Average (excluding any) f1 score: 0.5450\n",
      "Elapsed time was 0:37.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "model = LinearSVC(random_state=seed)\n",
    "_ = multi_cv(model, merge_features(training_comments, df, new_features), df_targets)\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine with Naive Bayes Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NBFeatures()\n",
    "nb.fit(training_comments, df_targets.any_label)\n",
    "nb_eng = NBFeatures()\n",
    "nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini test: Does feature scaling make a difference? Support vector machines are particularly vulnerable to unbalanced features, and I want to check whether scaling after the added step of the Naive Bayes feature transformation makes a difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic f1 score: 0.7636\n",
      "severe_toxic f1 score: 0.3483\n",
      "obscene f1 score: 0.7863\n",
      "threat f1 score: 0.3374\n",
      "insult f1 score: 0.6707\n",
      "identity_hate f1 score: 0.3474\n",
      "any_label f1 score: 0.7804\n",
      "Average (excluding any) f1 score: 0.5423\n",
      "Elapsed time was 3:14.\n"
     ]
    }
   ],
   "source": [
    "# Unscaled \n",
    "start = time.time()\n",
    "model = LinearSVC(random_state=seed)\n",
    "sc = StandardScaler(with_mean=False)\n",
    "_ = multi_cv(model, nb.transform(training_comments), df_targets) \n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toxic f1 score: 0.7636\n",
    "severe_toxic f1 score: 0.3483\n",
    "obscene f1 score: 0.7863\n",
    "threat f1 score: 0.3374\n",
    "insult f1 score: 0.6707\n",
    "identity_hate f1 score: 0.3474\n",
    "any_label f1 score: 0.7804\n",
    "Average (excluding any) f1 score: 0.5423\n",
    "Elapsed time was 3:14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled start = time.time()\n",
    "model = LinearSVC(random_state=seed)\n",
    "sc = StandardScaler(with_mean=False)\n",
    "_ = multi_cv(model, sc.fit_transform(nb.transform(training_comments)), df_targets)\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toxic f1 score: 0.6450\n",
    "severe_toxic f1 score: 0.1992\n",
    "obscene f1 score: 0.5849\n",
    "threat f1 score: 0.2039\n",
    "insult f1 score: 0.4803\n",
    "identity_hate f1 score: 0.1735\n",
    "any_label f1 score: 0.6640\n",
    "Average (excluding any) f1 score: 0.3811\n",
    "Elapsed time was 7:08."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling produced worse scores and took longer. I expected this, but just wanted to do a sanity check to validate my assumptions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With engineered features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic f1 score: 0.7696\n",
      "severe_toxic f1 score: 0.3623\n",
      "obscene f1 score: 0.7869\n",
      "threat f1 score: 0.3408\n",
      "insult f1 score: 0.6727\n",
      "identity_hate f1 score: 0.3488\n",
      "any_label f1 score: 0.7842\n",
      "Average (excluding any) f1 score: 0.5468\n",
      "Elapsed time was 3:37.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model = LinearSVC(random_state=seed)\n",
    "_ = multi_cv(model, nb_eng.transform(merge_features(training_comments, df, new_features)), df_targets)\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/lightgbm/engine.py:390: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time was 5:05.\n",
      "Final CV F1 score is 0.7470\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_data = lgb.Dataset(training_comments, label=df_targets.any_label.values)\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'verbose': 1,\n",
    "    'num_leaves': 64,\n",
    "    'n_estimators': 500, \n",
    "    'learning_rate': 0.05, \n",
    "    'max_depth': 16,\n",
    "    'n_jobs': -1,\n",
    "    'seed': seed\n",
    "}\n",
    "\n",
    "cv_results = lgb.cv(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=100,\n",
    "        nfold=5,\n",
    "        metrics='mae',\n",
    "        #early_stopping_rounds=10,\n",
    "        feval=lgb_f1_score\n",
    "        )\n",
    "print_time(start)\n",
    "\n",
    "print(\"Final CV F1 score is %.4f\" % cv_results['f1-mean'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elapsed time was 5:05.\n",
    "Final CV F1 score is 0.7470"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With engineered features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/lightgbm/engine.py:390: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time was 4:58.\n",
      "Final CV F1 score is 0.7573\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_data = lgb.Dataset(merge_features(training_comments, df, new_features), label=df_targets.any_label.values)\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'verbose': 1,\n",
    "    'num_leaves': 64,\n",
    "    'n_estimators': 500, \n",
    "    'learning_rate': 0.05, \n",
    "    'max_depth': 16,\n",
    "    'n_jobs': -1,\n",
    "    'seed': seed\n",
    "}\n",
    "\n",
    "cv_results = lgb.cv(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=100,\n",
    "        nfold=5,\n",
    "        metrics='mae',\n",
    "        #early_stopping_rounds=10,\n",
    "        feval=lgb_f1_score\n",
    "        )\n",
    "print_time(start)\n",
    "\n",
    "print(\"Final CV F1 score is %.4f\" % cv_results['f1-mean'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elapsed time was 5:04.\n",
    "Final CV F1 score is 0.7573"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:  Optimize tf-idf max features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time was 0:19.\n",
      "(127656, 10000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.784174616909\n",
      "Elapsed time was 0:33.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "comment_vector = TfidfVectorizer(max_features=10000, analyzer='word', #ngram_range=(3, 7), \n",
    "                                 stop_words='english')\n",
    "training_comments = comment_vector.fit_transform(df.comment_text)\n",
    "holdout_comments = comment_vector.transform(holdout.comment_text)\n",
    "submission_comments = comment_vector.transform(df_sub.comment_text)\n",
    "print_time(start)\n",
    "\n",
    "print(training_comments.shape)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "nb_eng = NBFeatures()\n",
    "nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n",
    "training_comments = nb_eng.transform(merge_features(training_comments, df, new_features))\n",
    "\n",
    "model = LinearSVC(random_state=seed)\n",
    "\n",
    "score = np.mean(cross_val_score(model, training_comments, df_targets.any_label, scoring='f1', cv=cv))\n",
    "\n",
    "print(score)\n",
    "\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.784174616909\n",
    "Elapsed time was 0:33."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time was 0:09.\n",
      "(127656, 30000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.792595764561\n",
      "Elapsed time was 0:21.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "comment_vector = TfidfVectorizer(max_features=30000, analyzer='word', # ngram_range=(3, 7), \n",
    "                                 stop_words='english')\n",
    "training_comments = comment_vector.fit_transform(df.comment_text)\n",
    "#holdout_comments = comment_vector.transform(holdout.comment_text)\n",
    "#submission_comments = comment_vector.transform(df_sub.comment_text)\n",
    "print_time(start)\n",
    "\n",
    "print(training_comments.shape)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "nb_eng = NBFeatures()\n",
    "nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n",
    "training_comments = nb_eng.transform(merge_features(training_comments, df, new_features))\n",
    "\n",
    "model = LinearSVC(random_state=seed)\n",
    "\n",
    "score = np.mean(cross_val_score(model, training_comments, df_targets.any_label, scoring='f1', cv=cv))\n",
    "\n",
    "print(score)\n",
    "\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.792595764561\n",
    "Elapsed time was 0:21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time was 0:38.\n",
      "(127656, 30000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787710758788\n",
      "Elapsed time was 0:25.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "comment_vector = TfidfVectorizer(max_features=30000, analyzer='word', ngram_range=(1,2), \n",
    "                                 stop_words='english')\n",
    "training_comments = comment_vector.fit_transform(df.comment_text)\n",
    "#holdout_comments = comment_vector.transform(holdout.comment_text)\n",
    "#submission_comments = comment_vector.transform(df_sub.comment_text)\n",
    "print_time(start)\n",
    "\n",
    "print(training_comments.shape)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "nb_eng = NBFeatures()\n",
    "nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n",
    "training_comments = nb_eng.transform(merge_features(training_comments, df, new_features))\n",
    "\n",
    "model = LinearSVC(random_state=seed)\n",
    "\n",
    "score = np.mean(cross_val_score(model, training_comments, df_targets.any_label, scoring='f1', cv=cv))\n",
    "\n",
    "print(score)\n",
    "\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.787710758788\n",
    "Elapsed time was 0:25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time was 3:34.\n",
      "(127656, 30000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.404212938192\n",
      "Elapsed time was 0:31.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "comment_vector = TfidfVectorizer(max_features=30000, analyzer='word', ngram_range=(2,6), \n",
    "                                 stop_words='english')\n",
    "training_comments = comment_vector.fit_transform(df.comment_text)\n",
    "#holdout_comments = comment_vector.transform(holdout.comment_text)\n",
    "#submission_comments = comment_vector.transform(df_sub.comment_text)\n",
    "print_time(start)\n",
    "\n",
    "print(training_comments.shape)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "nb_eng = NBFeatures()\n",
    "nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n",
    "training_comments = nb_eng.transform(merge_features(training_comments, df, new_features))\n",
    "\n",
    "model = LinearSVC(random_state=seed)\n",
    "\n",
    "score = np.mean(cross_val_score(model, training_comments, df_targets.any_label, scoring='f1', cv=cv))\n",
    "\n",
    "print(score)\n",
    "\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.404212938192\n",
    "Elapsed time was 0:31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time was 7:04.\n",
      "(127656, 20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.774235174963\n",
      "Elapsed time was 3:09.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "comment_vector = TfidfVectorizer(max_features=20000, analyzer='char', ngram_range=(3, 7), \n",
    "                                 stop_words='english')\n",
    "training_comments = comment_vector.fit_transform(df.comment_text)\n",
    "#holdout_comments = comment_vector.transform(holdout.comment_text)\n",
    "#submission_comments = comment_vector.transform(df_sub.comment_text)\n",
    "print_time(start)\n",
    "\n",
    "print(training_comments.shape)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "nb_eng = NBFeatures()\n",
    "nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n",
    "training_comments = nb_eng.transform(merge_features(training_comments, df, new_features))\n",
    "\n",
    "model = LinearSVC(random_state=seed)\n",
    "\n",
    "score = np.mean(cross_val_score(model, training_comments, df_targets.any_label, scoring='f1', cv=cv))\n",
    "\n",
    "print(score)\n",
    "\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.774235174963\n",
    "Elapsed time was 3:09."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time was 7:25.\n",
      "(127656, 10000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.796185423779\n",
      "Elapsed time was 2:56.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "word_vectorizer = TfidfVectorizer(max_features=5000, analyzer='word',# ngram_range=(1, 2), \n",
    "                                 stop_words='english')\n",
    "char_vectorizer = TfidfVectorizer(max_features=5000, analyzer='char', ngram_range=(3, 7), \n",
    "                                 stop_words='english')\n",
    "vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=-1)\n",
    "training_comments = vectorizer.fit_transform(df.comment_text)\n",
    "print_time(start)\n",
    "print(training_comments.shape)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "nb_eng = NBFeatures()\n",
    "nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n",
    "training_comments = nb_eng.transform(merge_features(training_comments, df, new_features))\n",
    "\n",
    "model = LinearSVC(random_state=seed)\n",
    "\n",
    "score = np.mean(cross_val_score(model, training_comments, df_targets.any_label, scoring='f1', cv=cv))\n",
    "\n",
    "print(score)\n",
    "\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.796185423779\n",
    "Elapsed time was 2:56."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time was 7:30.\n",
      "(127656, 20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798518003383\n",
      "Elapsed time was 2:07.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "word_vectorizer = TfidfVectorizer(max_features=15000, analyzer='word', ngram_range=(1, 2), \n",
    "                                 stop_words='english')\n",
    "char_vectorizer = TfidfVectorizer(max_features=5000, analyzer='char', ngram_range=(3, 7), \n",
    "                                 stop_words='english')\n",
    "vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=-1)\n",
    "training_comments = vectorizer.fit_transform(df.comment_text)\n",
    "print_time(start)\n",
    "print(training_comments.shape)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "nb_eng = NBFeatures()\n",
    "nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n",
    "training_comments = nb_eng.transform(merge_features(training_comments, df, new_features))\n",
    "\n",
    "model = LinearSVC(random_state=seed)\n",
    "\n",
    "score = np.mean(cross_val_score(model, training_comments, df_targets.any_label, scoring='f1', cv=cv))\n",
    "\n",
    "print(score)\n",
    "\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.798518003383\n",
    "Elapsed time was 2:07."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time was 7:29.\n",
      "(127656, 30000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800512138358\n",
      "Elapsed time was 2:36.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "word_vectorizer = TfidfVectorizer(max_features=20000, analyzer='word', ngram_range=(1, 2), \n",
    "                                 stop_words='english')\n",
    "char_vectorizer = TfidfVectorizer(max_features=10000, analyzer='char', ngram_range=(3, 7), \n",
    "                                 stop_words='english')\n",
    "vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=-1)\n",
    "training_comments = vectorizer.fit_transform(df.comment_text)\n",
    "print_time(start)\n",
    "print(training_comments.shape)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "nb_eng = NBFeatures()\n",
    "nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n",
    "training_comments = nb_eng.transform(merge_features(training_comments, df, new_features))\n",
    "\n",
    "model = LinearSVC(random_state=seed)\n",
    "\n",
    "score = np.mean(cross_val_score(model, training_comments, df_targets.any_label, scoring='f1', cv=cv))\n",
    "\n",
    "print(score)\n",
    "\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.800512138358\n",
    "Elapsed time was 2:36."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time was 3:03.\n",
      "(127656, 30000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80146407813\n",
      "Elapsed time was 2:48.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "word_vectorizer = TfidfVectorizer(max_features=20000, analyzer='word', ngram_range=(1, 2), \n",
    "                                 stop_words='english')\n",
    "char_vectorizer = TfidfVectorizer(max_features=10000, analyzer='char', ngram_range=(3, 5), \n",
    "                                 stop_words='english')\n",
    "vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=-1)\n",
    "training_comments = vectorizer.fit_transform(df.comment_text)\n",
    "print_time(start)\n",
    "print(training_comments.shape)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "nb_eng = NBFeatures()\n",
    "nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n",
    "training_comments = nb_eng.transform(merge_features(training_comments, df, new_features))\n",
    "\n",
    "model = LinearSVC(random_state=seed)\n",
    "\n",
    "score = np.mean(cross_val_score(model, training_comments, df_targets.any_label, scoring='f1', cv=cv))\n",
    "\n",
    "print(score)\n",
    "\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.80146407813\n",
    "Elapsed time was 2:48."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Optimize NB Feature Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be an issue with the consistency of scores here. The variation in training time suggests that the support vector machine algorithm is struggling with a too-large range of input values. Using the the last and best training_comments from the cell above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time was 2:51.\n",
      "(127656, 30000)\n",
      "**********************\n",
      "For epsilon 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon 0.100000 score: 0.7966\n",
      "Elapsed time was 1:03.\n",
      "**********************\n",
      "For epsilon 0.200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon 0.200000 score: 0.7989\n",
      "Elapsed time was 1:05.\n",
      "**********************\n",
      "For epsilon 0.300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon 0.300000 score: 0.8003\n",
      "Elapsed time was 1:08.\n",
      "**********************\n",
      "For epsilon 0.400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon 0.400000 score: 0.8001\n",
      "Elapsed time was 1:17.\n",
      "**********************\n",
      "For epsilon 0.500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon 0.500000 score: 0.8008\n",
      "Elapsed time was 1:28.\n",
      "**********************\n",
      "For epsilon 0.600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon 0.600000 score: 0.8013\n",
      "Elapsed time was 1:47.\n",
      "**********************\n",
      "For epsilon 0.700000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon 0.700000 score: 0.8007\n",
      "Elapsed time was 2:06.\n",
      "**********************\n",
      "For epsilon 0.800000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon 0.800000 score: 0.8020\n",
      "Elapsed time was 2:19.\n",
      "**********************\n",
      "For epsilon 0.900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon 0.900000 score: 0.8019\n",
      "Elapsed time was 2:36.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "word_vectorizer = TfidfVectorizer(max_features=20000, analyzer='word', ngram_range=(1, 2), \n",
    "                                 stop_words='english')\n",
    "char_vectorizer = TfidfVectorizer(max_features=10000, analyzer='char', ngram_range=(3, 5), \n",
    "                                 stop_words='english')\n",
    "vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=-1)\n",
    "training_comments = vectorizer.fit_transform(df.comment_text)\n",
    "print_time(start)\n",
    "print(training_comments.shape)\n",
    "\n",
    "for i in range(1,10):\n",
    "    start = time.time()\n",
    "    sc = StandardScaler(with_mean=False)\n",
    "    epsilon = i/10\n",
    "    print('**********************')\n",
    "    print('For epsilon %f' % epsilon)\n",
    "    nb_temp = NBFeatures(epsilon=epsilon)\n",
    "    nb_temp.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n",
    "    input_data = nb_temp.transform(merge_features(training_comments, df, new_features))\n",
    "    model = LinearSVC(random_state=seed)\n",
    "    score = np.mean(cross_val_score(model, input_data, df_targets.any_label, scoring='f1', cv=cv))\n",
    "    print('Epsilon %f score: %.4f' % (epsilon, score))\n",
    "    print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon 0.100000 score: 0.7966\n",
    "Elapsed time was 1:03.\n",
    "**********************\n",
    "Epsilon 0.200000 score: 0.7989\n",
    "Elapsed time was 1:05.\n",
    "**********************\n",
    "Epsilon 0.300000 score: 0.8003\n",
    "Elapsed time was 1:08.\n",
    "**********************\n",
    "Epsilon 0.400000 score: 0.8001\n",
    "Elapsed time was 1:17.\n",
    "**********************\n",
    "Epsilon 0.500000 score: 0.8008\n",
    "Elapsed time was 1:28.\n",
    "**********************\n",
    "Epsilon 0.600000 score: 0.8013\n",
    "Elapsed time was 1:47.\n",
    "**********************\n",
    "Epsilon 0.700000 score: 0.8007\n",
    "Elapsed time was 2:06.\n",
    "**********************\n",
    "Epsilon 0.800000 score: 0.8020\n",
    "Elapsed time was 2:19.\n",
    "**********************\n",
    "Epsilon 0.900000 score: 0.8019\n",
    "Elapsed time was 2:36.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: SVM Parameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time was 2:59.\n",
      "(127656, 30000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "word_vectorizer = TfidfVectorizer(max_features=20000, analyzer='word', ngram_range=(1, 2), \n",
    "                                 stop_words='english')\n",
    "char_vectorizer = TfidfVectorizer(max_features=10000, analyzer='char', ngram_range=(3, 5), \n",
    "                                 stop_words='english')\n",
    "vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=-1)\n",
    "training_comments = vectorizer.fit_transform(df.comment_text)\n",
    "print_time(start)\n",
    "print(training_comments.shape)\n",
    "\n",
    "# Reset NB feature transformer epsilon value\n",
    "nb_eng = NBFeatures()\n",
    "nb_eng.fit(merge_features(training_comments, df, new_features), df_targets.any_label)\n",
    "input_data = nb_eng.transform(merge_features(training_comments, df, new_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "model = SVC(random_state=seed)\n",
    "score = np.mean(cross_val_score(model, training_comments, df_targets.any_label, scoring='f1'))\n",
    "\n",
    "print(score)\n",
    "\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test C (L2 Penalization Coefficient) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0\n",
      "0.804947502199\n",
      "Elapsed time was 1:07.\n",
      "C: 0\n",
      "0.805441509001\n",
      "Elapsed time was 1:32.\n",
      "C: 1\n",
      "0.80322039905\n",
      "Elapsed time was 2:15.\n"
     ]
    }
   ],
   "source": [
    "params = [0.3, 0.5, 0.8]\n",
    "\n",
    "for C in params:\n",
    "    print(\"C: %.f\" % C)\n",
    "    start = time.time() \n",
    "    model = LinearSVC(random_state=seed, C=C)\n",
    "    score = np.mean(cross_val_score(model, input_data, df_targets.any_label, scoring='f1', cv=cv))\n",
    "    print(score)\n",
    "    print_time(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimimum Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:25: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time was 3:29.\n",
      "Elapsed time was 0:19.\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "start = time.time()\n",
    "word_vectorizer = TfidfVectorizer(max_features=20000, analyzer='word', ngram_range=(1, 2), \n",
    "                                 stop_words='english')\n",
    "char_vectorizer = TfidfVectorizer(max_features=10000, analyzer='char', ngram_range=(3, 5), \n",
    "                                 stop_words='english')\n",
    "vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=-1)\n",
    "\n",
    "# Fit to and transform input data\n",
    "X_train = vectorizer.fit_transform(df.comment_text)\n",
    "X_test = vectorizer.transform(holdout.comment_text)\n",
    "\n",
    "# Name training target data\n",
    "y_train = df_targets.any_label\n",
    "y_test = holdout_targets.any_label\n",
    "\n",
    "# Create and fit NB Feature extractor \n",
    "nb = NBFeatures()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Tranform input data\n",
    "X_train = nb.transform(X_train)\n",
    "X_test = nb.transform(X_test)\n",
    "\n",
    "print_time(start)\n",
    "\n",
    "# Define model and fit to data \n",
    "start = time.time()\n",
    "model = LinearSVC(random_state=seed, C=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print_time(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 2386\n",
      "False Positives: 282\n",
      "True Negatives: 28389\n",
      "False Negatives: 858\n",
      "Precision: 0.8943\n",
      "Recall: 0.7355\n",
      "F1 Score: 0.8072\n",
      "Total Accuracy: 0.96%\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "acc = (tp+tn)/(tn+fn+tp+fp)\n",
    "print(\"True Positives: %d\" % tp)\n",
    "print(\"False Positives: %d\" % fp)\n",
    "print(\"True Negatives: %d\" % tn)\n",
    "print(\"False Negatives: %d\" % fn)\n",
    "print(\"Precision: %.4f\" % (tp/(tp+fp)))\n",
    "print(\"Recall: %.4f\" % (tp/(tp+fn)))\n",
    "print(\"F1 Score: %.4f\" % f1_score(y_test, y_pred))\n",
    "print(\"Total Accuracy: %.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "def cm_heatmap(arr, title):\n",
    "    \"\"\"Internal. Only called by scoring function.\"\"\"\n",
    "    plt.figure('cm_heatmap', figsize=(10,10))\n",
    "    plt.title(title + ' confusion matrix')\n",
    "    sns.heatmap(arr, square=True, annot=True, cmap='YlOrRd', fmt='g', cbar=False)\n",
    "    plt.xlabel(\"Truth\")\n",
    "    plt.ylabel(\"Prediction\")\n",
    "    #sns.set(font_scale=3)\n",
    "    plt.show()\n",
    "cm_heatmap(cm, \"Toxic Comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "kaYNMVqj5yxo",
    "yH8NDq2D5yx9",
    "jxMujLal5yyN",
    "KmQxwcqp5yyu",
    "_A0tzkGe5yyz",
    "41hcbO6m5yy8",
    "qZU1wqtf5yy9",
    "WPNxOBNp5yzJ",
    "QeJWnVa15yzZ",
    "ZSCnkLiC5y0A",
    "krzeQWNb5y0h",
    "X7Yxshbs5y02",
    "1O2erv3G5y07",
    "lw2pO8mo5y0-",
    "aCDEfw7h5y1F",
    "VJj2r6ky5y1O",
    "5yiO9G4V5y1U",
    "c_NzviMo5y1t",
    "fVAFqZCz5y2P",
    "6dEs1c0H5y2r"
   ],
   "default_view": {},
   "name": "ToxicComments.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
